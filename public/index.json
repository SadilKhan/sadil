[{"categories":["Computer Vision","Deep Learning","Point Cloud","Segmentation","Graph","Voxel","MLP"],"contents":" 1. What is Point Cloud? A Point Cloud is a set of points in 3D space which can represent the boundary or the whole object (including inside points). In a point cloud, the points are unordered and are not restricted by any grid which means a point cloud can be expressed in an infinite way (using translation). Each point can have 3D coordinates and feature vectors. $$ P={(X_i,F_i)}^{i=N}_{i=1}, X_i\\in\\mathbb{R}^3,F_i\\in\\mathbb{R}^d$$\n2. Properties of Point Cloud in $\\mathbb{R}^3$ Unordered: Unlike images or arrays, point cloud is unordered. It has no restriction to be confined within a boundary. This causes a problem for CNN type architecture to learn since CNN uses convolutional operations which requires ordered and regular array like representation of the input. Point cloud networks are generally invariant to the $N!$ number of permutations in input.\nIrregularity: Points are not sampled uniformly from an image which means different objects can have dense points while others sparse [1, 2]. This sometimes causes class imbalance problems in point cloud dataset.\nConnectedness: Since points are not connected like graph structure and neighbouring points contain meaningful spatial and geometry information of the object, networks must learn to pass information from points to points.\n3. Point Cloud Generation Point clouds are generated by 3D Scanners like time-of-flight sensors and depth cameras or photogrammetry software. Time-of-flight sensors use the reflected laser beams from sensors to the object to capture the surface of the object.\n4. Point Cloud Sampling Point Cloud Sampling is the method of choosing a subset of point clouds from a large point cloud set. Sampling methods were used in segmentation model to reduce the number of points for faster learning [RandLa-Net]. This is an essential step in the large-scale point cloud processing, since learning features for all the points can be time consuming. Instead, features can be learnt for small point clouds and for other points, it can be aggregated using neighboring features. There are different sampling algorithms available. Let $N$ be the number of points, $M$ is the sampled number of points chosen with $N\u003eM$, $D$ is the maximum number of points in a 3D voxel grid ($N\u003e\u003eD$) and $K$ is the number of nearest neighbour($N\u003e\u003eK$). $\\textbf{1. Heuristic Sampling}$ Grid Sampling: In Grid Sampling, a 3D voxel grid is used over the point cloud and each occupied voxels extract one point based on averages or most frequently occurring classes. This sampling results in a uniform sample. The time complexity of the grid sampling is $O(ND)$. By averaging the points on the surface, grid sampling loses smooth boundary information. Random Sampling: One of the simplest sampling methods, Random Sampling takes $M$ random points from a point cloud of $N$ points ($N\u003eM$). Time complexity is $O(M)$ which makes it efficient to use in large-scale point cloud networks. Farthest Point Sampling(FPS): It iteratively extracts set of points $P=\\{p_1,p_2,\\cdots,p_M \\}$ such that $p_j$ is the farthest point from the first $j-1$ points in $P$. The time complexity is $O(M^2N)$ which makes it unsuitable for large scale point cloud processing. Inverse Density Importance Sampling: In IDIS, density is calculated for every point by adding the distance between the point and its nearest neighbors. $$density(x)=\\sum_{y\\in KNN(x)} \\lVert x-y \\rVert_2^2$$. So $N$ points are reordered according to the inverse of the density and top $M$ points are selected which means lower density points are more likely to be chosen than high dense points. Time complexity is $O((K+N)logN)$. This sampling can control density but is sensitive to outliers and noise. $\\textbf{2. Learning Based Sampling}$ Generator Based Sampling: Generator Based Sampling(GS) learns to generate a small subset of point clouds from the original point cloud. For a point cloud set $P$ and a task $T$, GS tries to find $S \\subset P$ by minimizing the objective function $f$ such that $$S^*=argmin_{S}(f(T(S))$$. It is an end-to-end trainable model. But at inference stage, it uses FPS to match subsets with original point cloud. It takes up to 20 minutes to sample 10% of $10^6$ points. Gumbel Subset Sampling: Gumbel Subset Sampling[4] uses attention mechanism to choose a representative and task-specific subset of the point cloud. Given an input set $X_i \\in \\mathbb{R}^{N_i\\times c}$, the task is to choose a suitable $X_{i+1} \\in \\mathbb{R}^{N_{i+1}\\times c}, N_{i+1} \\leq N_i$ and $$X_{i+1}=y\\cdot softmax(WX_i^T), W \\in \\mathbb{R}^{N_{i+1}\\times N_i}$$ It is completely end-to-end learnable and can be used in any segmentation network. 5. Bibliography Anh Nguyen, Bac Le, 3D Point Cloud Segmentation - A Survey, 2013 6th IEEE Conference on Robotics, Automation and Mechatronics (RAM), 2013, pp. 225-230.\nCharles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas, PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 77-85.\nQingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, A. Trigoni, A. Markham, RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nJiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, Qi Tian, Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 3318-3327. 10.1109/CVPR.2019.00344. M. Fan. Variants of Seeded Region Growing. Image Processing, IET · June 2015\nHang Su, Subhransu Maji ,Evangelos Kalogerakis, Erik Learned-Miller. Multi-view Convolutional Neural Networks for 3D Shape Recognition. 2015 IEEE International Conference on Computer Vision (ICCV), 2015, pp. 945-953 Saifullahi Aminu Bello , Shangshu Yu, Cheng Wang. Review: deep learning on 3D point clouds. Remote Sensing 12, No. 11:1729. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou. Tangent Convolutions for Dense Prediction in 3D. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Zhijian Liu, Haotian Tang, Yujun Lin, Song Han. Point-Voxel CNN for Efficient 3D Deep Learning. Proceedings of the 33rd International Conference on Neural Information Processing Systems 2019. Charles R. Qi, Li (Eric) Yi, Hao Su, Leonidas J. Guibas PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 5105–5114. H. Zhao, L. Jiang, C. -W. Fu and J. Jia PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 5560-5568, doi: 10.1109/CVPR.2019.00571. ","permalink":"/blog/pointcloud-series/introduction/","tags":null,"title":"Part 1 - Point Cloud Introduction"},{"categories":["Computer Vision","Deep Learning","Point Cloud","Segmentation","Graph","Voxel","MLP"],"contents":" Point Cloud Segmentation Methods Point Cloud Segmentation is the task for grouping objects or assigning labels to every points in the point cloud. It is one of the most challenging tasks and a research topic in deep learning since point clouds are noisy, unstructured and lack connectedness property. All the methods are categorized into four categories.\n1. Edge Based Methods Edges describe the intrinsic characteristics of the boundary of any 3D object. Edge-based methods locate the points which have rapid changes in the neighborhood. Bhanu[1] proposed three approaches for detecting the edges of a 3D object. The first approach is calculating the gradient. Let $r(i,j)$ be the range value at $(i,j)$ position, the magnitude and the direction of edge can be calculated by $$m(i,j:0)=\\frac{r(i,j-k)+r(i,j+k)-2r(i,j)}{2k}$$ $$m(i,j;45)=\\frac{r(i-k,j+k)+r(i+k,j-k)-2r(i,j)}{2k\\sqrt2}$$ $$m(i,j;90)=\\frac{r(i-k,j)+r(i+k,j)-2r(i,j)}{2k}$$ $$m(i,j;135)=\\frac{r(i-k,j-k)+r(i+k,j+k)-2r(i,j)}{2k\\sqrt2}$$\nFor flat surfaces these values are zero, positive when edges are convex and negative when edges are concave. The maximum magnitude of gradient is $\\max m(i,j;\\theta)$ and the direction of edge is $argmax_{\\theta}$ $m(i,j;\\theta)$. Using threshold, points can be segmented. The second approach is fitting 3D lines to a set of points(i.e neighboring points) and detecting the changes in the unit direction vector from a point to the neighboring points. The third approach is a surface normal approach where changes in the normal vectors in the neighborhood of a point determine the edge point. Edge models are fast and interpretable but they are very sensitive to noise and sparse density of point clouds and lack generalization capability. Learning on incomplete point cloud structure with edge-based models does not give good accuracy. In Medical image datasets especially MRI data, the organ boundaries sometimes do not have high gradient points compared to CT data which means for every modality, we have to find new thresholds in edge-based methods.\n2. Region Based Methods Region-based methods use the idea of neighborhood information to group points that are similar thus finding similarly grouped 3D objects and maximizing the dissimilarity between different objects. Compared to edge-based methods, these methods are not susceptible to noise and outliers but they suffer from inaccurate border segmentation. There are two types of region-based methods.\nSeeded-region Methods(bottom up): Seeded region segmentation is a fast, effective and very robust image segmentation method. It starts the segmentation process by choosing manually or automatically in preprocessing step, a set of seeds which can be a pixel or a set of pixels and then gradually adding neighbouring points if certain conditions satisfy regarding similarity[1,5]. The process finishes when every point belongs to a region. Suppose there are N seeds chosen initially. Let $A=\\{A_1,A_2,\\cdots,A_N\\}$ be the set of seeds. Let T be the set of pixels that are not in any $A_i$ but is adjacent to at least a point in $A_i$. $$T = \\bigg\\{x\\notin \\bigcup_{i=1}^{i=N} A_i|nbr(x) \\cap \\bigcup_{i=1}^{i=N} A_i \\neq \\phi \\bigg\\}$$ where $nbr(x)$ is the neighbourhood points of x. At each step if $nbr(x) \\cap A_i \\neq \\phi$, then x is added into the region if certain conditions are met. One such condition can be checking the difference between intensity value of $x$ with the average intensity value of $A_i \\forall A_i \\text{ such that } nbr(x) \\cap A_i \\neq \\phi$. The region with minimum difference is assigned to the point. There are another method when greyvalues of any point is approximated by fitting a line i.e if a coordinate of any pixel/point $p$ is $(x,y)$, then greyvalue of $p$, $G(p)=b+a_1x+a_2y+\\epsilon$, where $\\epsilon$ is the error term. The new homogeneity condition is to find the minimum distance between average approximated greyvalue and the approximated greyvalue of $x$. Seeded-based segmentation is very much dependent upon the choice of seed points. Inaccurate choices often lead to under-segmentation or over-segmentation. Unseeded-region Methods(top-down): Unlike seeded-based methods, unseeded methods have a top-down approach. The segmentation starts with grouping all the points into one region. Then the difference between all the mean point values and chosen point value is calculated. If it is more than the threshold then the point is kept otherwise the point is different than the rest of the points and a new region is created and the point is added into the new region and removed from the old region. The challenges are over-segmentation and domain-knowledge which is not present in complex scenes[1]. 3. Attribute Based Methods Attribute-based methods use the idea of clustering. The approach is to calculate attributes for points and then use a clustering algorithm to perform segmentation. The challenges in these methods are how to find a suitable attribute that contains the necessary information for segmentation and to define proper distance metrics. Some of the attributes can be normal vectors, distance, point density, or surface texture measures. It is a very robust method but performs poorly if points are large-scale and attributes are multidimensional.[1]\n4. Deep Learning Based Methods The main challenge in point cloud segmentation is find good latent vector which can contain sufficient information for segmentation task. Deep Learning methods offers the best solution to learn good representations. Neural networks being a universal approximator can theoretically approximate the target function for segmentation. The following theorem justifies how MLPs can approximate the function for the segmentation task given enough neurons.\nTheorem 1: Given a set of point clouds $X=\\{\\{x_i\\}_{i=1}^{i=n},n\\in \\mathbb{Z}^+,x_i \\in [0,1]^m\\} $, let $f:X \\rightarrow R$ be a continuous function with respect to hausdorff distance($d_H(\\cdot,\\cdot)$).$\\forall \\epsilon \u003e 0, \\exists \\eta,$ a continuous function and a symmetric set function $g(x_1,x_2,\\cdots x_n)=\\gamma \\circ MAX$ such that $\\forall S\\subset X$. $$\\bigg|f(S)-\\gamma \\bigg(\\underset{x_i \\in S}{MAX}(\\eta(x_i)) \\bigg) \\bigg|\u003c\\epsilon$$ $\\gamma$ is a continuous function and $MAX$ is an elementwise max operation which takes an input $k$ number of vectors and return a vector with element wise maximum. In practice $\\gamma \\text{ and } \\eta$ are MLP [2].\nProof: The hausdorff distance is defined by $$d_H(x,y)=\\max\\bigg\\{\\sup\\limits_{x\\in X}(\\inf\\limits_{y\\in Y} d(x,y)), \\sup\\limits_{y\\in Y}(\\inf\\limits_{x\\in X} d(x,y))\\bigg\\}$$ Since $f$ is a continuous function from $Y$ to $R$ w.r.t hausdorff distance, so by definition of continuity $\\forall \\epsilon \u003e 0, \\exists \\delta_{\\epsilon} \u003e 0 $ such that if $S_1,S_2 \\subset X$ and $d_H(S_1,S_2)\u003c\\delta_{\\epsilon}$, then $|f(S_1)-f(S_2)|\u003c\\epsilon$. Let $K=\\lceil {\\frac{1}{\\delta_\\epsilon}}\\rceil, K\\in \\mathbb{Z}^+$. So $[0,1]$ is evenly divided into K intervals. Let $\\sigma(x)$ be defined by $$\\sigma(x) =\\frac{\\lfloor{Kx}\\rfloor}{K}, x \\in S$$ So $\\sigma$ maps a point to the left side of the interval it belongs to and $$|x-\\frac{\\lfloor{Kx}\\rfloor}{K}|=\\frac{Kx-\\lfloor{Kx}\\rfloor}{K}\u003c1/K\\leq \\delta_\\epsilon$$ Let $\\tilde{S}={\\sigma(x)},x\\in S$, then $$|f(S) - f(\\tilde{S})|\u003c\\epsilon$$ Since $d_H(S,\\tilde{S})\\leq \\delta_\\epsilon.$ Let $\\eta_k(x)=e^{-d(x,[\\frac{k-1}{k},\\frac{k}{K}])}$ be the indicator function where $d(x,I)$ is the point to set distance $$d(x,I)=\\inf\\limits_{y \\in I}d(x,y)$$ $d(x,I)=0$, if $x\\in I$, so $\\eta_k(x)=1 \\text{ if } x \\in [\\frac{k-1}{k},\\frac{k}{K}].$ Let $\\eta(x)=[\\eta_1(x);\\eta_2(x);\\cdots;\\eta_n(x)]$. Since there are K intervals we can define K functions $v_j:\\mathbb{R}^n \\rightarrow \\mathbb{R}, \\forall j=1,\\cdots,K$ such that $$v_j(x_1,x_2,x_3,\\cdots,x_n)=\\max\\{\\eta_j(x_1),\\cdots,\\eta_j(x_n)\\}$$ So $v_j$ denotes if any points from $S$ occupy the $jth$ interval. Let $v=[v_1;v_2,\\cdots,;v_n]$. So $v:R^n\\rightarrow [0,1]^K$. Let $\\tau:[0,1]^K \\rightarrow X$ be defined by $$\\tau(v(x_1,x_2,\\cdots,x_n))=\\bigg\\{\\frac{k-1}{K}: v_k \\geq 1 \\bigg\\}$$ So $\\tau$ denotes the lower bound of any interval if it contains any point from $S$. In this respect, $\\tau(v) \\equiv \\tilde{S}$. Let $range(\\tau(v))=S_{\\tau}, d_H(S_{\\tau},S)\u003c\\frac{1}{K} \\leq \\delta_{\\epsilon}$ $$|f(\\tau(v(x_1,x_2,\\cdots,x_n)))-f(S)|\u003c\\epsilon$$ Let $\\gamma:\\mathbb{R}^K \\rightarrow R$ be a continuous function such that $\\gamma(v)=f(\\tau(v))$.Now $$\\gamma(v(x_1,x_2,\\cdots,x_n))=\\gamma (MAX)(\\eta(x_1),\\cdots,\\eta(x_n))$$ So $f$ can be approximated by a continuous($\\gamma$) and a symmetric function($MAX$).In practice, $\\gamma \\text{ and } \\eta$ can be approximated by MLP.\nThe DL methods for point cloud segmentation can be divided into following ways.\nA. Projection-Based Networks Following the success of 2d CNNs, projection-based networks use the projection of 3D point clouds into 2d images from various views/angles. Then 2D CNN techniques are applied to it to learn feature representations and finally features are aggregated with multi-view information for final output [6,7]. In [8], tangent convolutions are used. For every point, tangent planes are calculated and tangent convolutions are based on the projection of local surface geometry on the tangent plane. This gives a tangent image which is an $l\\times l$ grid where 2d convolutions can be applied. Tangent images can be computed even on a large-scale point cloud with millions of points. Compared to voxel-based models, multi-view models perform better since 2D CNN is a well-researched area and multi-view data contain richer information than 3D voxels even after losing depth information. The main challenges in multi-view methods are the choice of projection plane and the occlusion which can affect accuracy.\nB. Voxel-Based Networks Voxel-based methods convert the 3D point clouds into voxel-based images. Figure [1] shows an example. The points which make up the point cloud are unstructured and unordered but CNN requires a regular grid for convolution operation.\nFigure 1: Voxelization of a point cloud (Image from [9]) Voxelization is done in the following steps. A bounding box of the point cloud is calculated which defines the entire space that is to be divided. Then the space is divided into a fixed-size grid. Each grid is called 3D cuboids. The point cloud is divided into different grids with each 3D cuboid containing several points and these 3D cuboids become voxels that represent the subset of points. Features are calculated from the subset of points inside a voxel. Voxelization creates quantization artifacts and loses smooth boundary information. It is a computationally expensive preprocessing step and memory footprints increase cubically due to the cubical growth of voxels. If voxel resolution is low, many points will belong to a voxel and will be represented by a single voxel so these points will not be differentiable . A point is differentiable if it exclusively occupies one voxel grid. Figure 2 summarizes the memory requirements for if we want to retain higher number of differentiable points which will mean lower information loss [9]. To retain 90% of the differentiable points, GPU memory is more than 82 GB and voxel resolution is $128 \\times 128 \\times 128$ which is a huge computational overload. Figure 2: Voxelization and memory footprint (Image from [9])) After voxelization, 3D CNNs can be applied for learning features for segmentation (3d UNet). In a similar approach, Point-Voxel CNN [9] uses CNN and MLP bases fusion learning. It first voxelizes the point cloud and uses convolution for feature learning and then devoxelize the voxels for voxel-to-point mapping(i.e interpolation is used to create distinct features of a voxel for the points that belong to the voxel). The features of a point cloud are then aggregated with the features learned using MLP. Despite its remarkable advances in segmentation tasks in the medical domain in segmentation tasks, 3D CNNs have a lot of parameters and is computationally expensive. Reducing the input size causes the loss of important information. 3DCNN also requires a large number of training samples. C. Point-Based Networks Point-Based Networks work on raw point cloud data. They do not require voxelization or projection. PointNet is a breakthrough network that takes input as raw point clouds and outputs labels for every point. It uses permutation-invariant operations like pointwise MLP and symmetric layer, Max-Pooling layer for feature aggregation layer. It achieves state-of-the-art performance on benchmark datasets. But PointNet lacks local dependency information and so it does not capture local information. The max-pooling layer captures the global structure and loses distinct local information. Inspired by PointNet many new networks are proposed to learn local structure. PointNet++ extends the PointNet architecture with an addition of local structure learning method. The local structure information passing idea follows the three basic steps (1) Sampling (2) Grouping (3) Feature Aggregation Layer (Section 3.3.1.E lists some Feature Aggregation functions) to aggregate the information from the points in the nearest neighbors. Sampling is choosing $M$ centroids from $N$ points in a point cloud ($N\u0026gt;M$). Random Sampling or Farthest Point Sampling are two such methods for sampling centroids. Grouping refers to sample representative points for a centroid using KNN. It takes the input (1) set of points $N\\times(d+C)$, with $N$ is the number of points,$d$ coordinates and $C$ feature dimension and (2) set of centroids $N_1\\times d$. It outputs $N_1\\times K \\times (d+C)$ with $K$ is the number of neighbors. These points are grouped in a local patch. The points in the local patches are used for creating local feature representation for centroid points. These local patches work like receptive fields. Feature Aggregation Layer takes the feature of the points in the receptive field and aggregate them to output $N_1\\times(d+C)$. This process is repeated in a hierarchical way reducing the number of points as it goes deeper. This hierarchical structure enables the network to be able to learn local structures with an expanding receptive field. Most of the research in this field has gone into developing an effective feature aggregation layer to capture local structures. PointWeb creates a new module Adaptive Feature Adjustment to enhance the neighbor features by adding the information about the impact of features on centroid features and the relation between the points. It then combines the features and uses MLP to create new representations for centroid points. Despite their initial successes the following methods achieve higher performance due to their advanced local aggregation operators.\n5. Bibliography Anh Nguyen, Bac Le, 3D Point Cloud Segmentation - A Survey, 2013 6th IEEE Conference on Robotics, Automation and Mechatronics (RAM), 2013, pp. 225-230.\nCharles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas, PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 77-85.\nQingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, A. Trigoni, A. Markham, RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nJiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, Qi Tian, Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 3318-3327. 10.1109/CVPR.2019.00344. M. Fan. Variants of Seeded Region Growing. Image Processing, IET · June 2015\nHang Su, Subhransu Maji ,Evangelos Kalogerakis, Erik Learned-Miller. Multi-view Convolutional Neural Networks for 3D Shape Recognition. 2015 IEEE International Conference on Computer Vision (ICCV), 2015, pp. 945-953 Saifullahi Aminu Bello , Shangshu Yu, Cheng Wang. Review: deep learning on 3D point clouds. Remote Sensing 12, No. 11:1729. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou. Tangent Convolutions for Dense Prediction in 3D. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Zhijian Liu, Haotian Tang, Yujun Lin, Song Han. Point-Voxel CNN for Efficient 3D Deep Learning. Proceedings of the 33rd International Conference on Neural Information Processing Systems 2019. Charles R. Qi, Li (Eric) Yi, Hao Su, Leonidas J. Guibas PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 5105–5114. H. Zhao, L. Jiang, C. -W. Fu and J. Jia PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 5560-5568, doi: 10.1109/CVPR.2019.00571. ","permalink":"/blog/pointcloud-series/segmentation/","tags":["Deep Learning","Point Cloud","Segmentation","Graph","Voxel","MLP"],"title":"Part 2 - Point Cloud Segmentation"},{"categories":["Image Segmentation","Deep Learning","3D Computer Vision","Point Cloud"],"contents":" Abstract In this report, we present a novel approach for 3D medical image segmentation using point clouds. 3D Convolutional Neural Networks have been the most dominating networks in medical image processing but they require large memory footprints and training samples. Hence we used point clouds to represent the image instead of voxels. Point clouds are lightweight and contain shape and smoother surface information. We extracted the point clouds from 3D voxel images using canny edge detection. We modified RandLa-Net, an attention-based point cloud segmentation network with a feature extraction layer to aggregate local geometrical features with spatial point features for our large-scale point cloud segmentation task. Our proposed model performed better than the original network in multi-class as well as binary point cloud segmentation tasks in Visceral dataset. Finally, we propose a model-independent step to perform the image segmentation of the original 3D volumetric images in Visceral dataset by mapping voxels in the point cloud space and adding it to the input point cloud before being passed to the trained model. We performed many experiments on the weights of the Cross-Entropy loss function for the class imbalance problem as well as the intrinsic architectural properties of the model architecture like downsampling factor and distinct latent vector learning that can be improved to perform better segmentation.\n","permalink":"/publications/masterthesis/","tags":null,"title":"Learning Shapes for Efficient Segmentation of 3D Medical Images using Point Cloud"},{"categories":["Deep Learning","Point Cloud","Segmentation","Encoder","Decoder"],"contents":" 1. Point Cloud A. Introduction A Point Cloud is a set of points in 3D space which can represent the boundary or the whole object (including inside points). In a point cloud, the points are unordered and are not restricted by any grid which means a point cloud can be expressed in an infinite way (using translation). Each point can have 3D coordinates and feature vectors. $$P={(X_i,F_i)}^{i=N}_{i=1}, X_i\\in\\mathbb{R}^3,F_i\\in\\mathbb{R}^d$$\nB. Properties of Point Cloud in $\\mathbb{R}^3$ $\\textit{Unordered:}$ Unlike images or arrays, point cloud is unordered. It has no restriction to be confined within a boundary. This causes a problem for CNN type architecture to learn since CNN uses convolutional operations which requires ordered and regular array like representation of the input. Point cloud networks are generally invariant to the $N!$ number of permutations in input. $\\textit{Irregularity:}$ Points are not sampled uniformly from an image which means different objects can have dense points while others sparse [1, 2]. This sometimes causes class imbalance problems in point cloud dataset. $\\textit{Connectedness:}$ Since points are not connected like graph structure and neighbouring points contain meaningful spatial and geometry information of the object, networks must learn to pass information from points to points. 2. RandLaNet - Architecture Large-scale point cloud segmentation is a challenging task because of huge computational requirements and effective embedding learning. RandLa-Net[3] is an efficient and lightweight neural architecture that segments every point in large-scale point clouds. It is an encoder-decoder-like architecture that uses random sampling to downsample the input point cloud in the encoder and upsample the point cloud in decoder blocks. It uses random sampling compared to other sampling methods because of faster computation. Although random sampling can discard key points necessary for efficient point cloud segmentation, RandLa-Net implements attention-based local feature aggregation to effectively share features of points that are removed into the neighbor points. Figure[1] is the architecture of RandLa-Net. Figure 1: RandLa-Net Architecture. FC is the fully connected layer, LFA is the localfeature aggregation, RS is random sampling, MLP is shared multilayer perceptron,US is upsampling and DP is dropout. (Image from [3]) A. Random Sampling Compared to other sampling methods, Random sampling is extremely fast (time complexity $O(N)$). It is invariant to any changes to the points as well as the permutation of points. The random-sampling block is added in encoder part. To compensate for the loss of information, the author has added LFA module. Figure 2: Random Sampling in RandLa-Net. The downsampling rate is a hyperparameter and has significant influence on model performance (Image from [3]) B. Architecture RandLa-Net consists of 4 encoder and 4 decoder layers (Figure 1). Each encoder layer consists of LFA modules (which is shown in the bottom panel of Figure 3). LFA modules aggregate the local features and gradually expands the receptive field to perform global feature passing. Every LFA module is followed by a random sampling step. Let the input shape be $N\\times d_n$, where $N$ is the number of points in the point clouds ($N\\approx 10^6 - 10^7$) and $d_n \\in \\mathbb{R^d},d\\geq3$). $d_n$ can contain the coordinates with other features like intensity, gradient or normal. $\\textbf{Positional Encoding:}$Since point clouds are unstructured, positional encoding layer embeds the positional information in an 8 dimensional vector ($3\\rightarrow 8$). This layer describes the location of a point by mapping the position/index of a point into a vector and assigning unique representation for every point. In this way, positional encoding layer makes the network more permutation-invariant. $\\textbf{Encoding Layer:}$ The encoding layer progressively reduces the number of points and increases the point features. The point cloud is downsampled at each encoding layer after the dilated residual block by downsampling factor 4. $$N\\rightarrow \\frac{N}{4} \\rightarrow \\frac{N}{4^2} \\rightarrow \\frac{N}{4^3} \\rightarrow \\frac{N}{4^4}$$ The per-point feature dimension is increased gradually. $$8 \\rightarrow 32 \\rightarrow 128 \\rightarrow 256 \\rightarrow 512$$ $\\textbf{Decoding Layer:}$ In each decoder layer, points are upsampled. In each encoder layer, when a point is removed, it is stored as a reference. In subsequent decoding layer, (i.e the layer with which a skip connection is added from an encoder in Figure 1 for each query reference point, KNN is used to find the one nearest neighbor in the input set of points. Afterwards, feature of the nearest point is copied to the target point. Subsequently, the feature maps are concatenated with the feature maps produced by corresponding encoding layers through skip connections. Then a shared MLP is applied to the concatenated feature maps. Shared MLP means same MLP network for every point in the input point cloud. $\\textbf{Final Output Layer:}$ The segmentation label is predicted through three fully connected layers $(N,64) \\rightarrow (N,32) \\rightarrow (N,C)$, where $C$ is the number of classes. 3. RandLaNet - LFA The Local Feature Aggregation follows a three-step message passing system. Since point cloud don't have connectivity information, LFA ensures features are shared between points. In Figure 1, the LFA module in the first encoder transforms the feature vector ($8 \\rightarrow 32$) and random sampling removes 75% of the points. Let's take a point in the first encoder $(p,f),p\\in \\mathbb{R}^3,f\\in \\mathbb{R}^8$. Figure 3: RandLaNet Feature Sharing Let's take an overview of how this happens before diving deep into it. $\\textbf{1. Sampling:}$ The first step in message passing system is from which points we want to pass a message to the red point $p$ in Figure 3. K-Nearest Neighbor is used to find $K$ neighbor points (blue points) which will share its features with red point $p$. $\\textbf{2. Message Generation:}$ Once we choose the points, we need to generate the message to send from blue points to red point. For every point, $p_i$, we will generate a message $f_i$ by incorporating the distance and spatial information using an MLP. This MLP will give us the desired dimension of feature vector for $f_i,\\forall i=1,2,\\cdots,K$. $\\textbf{3. Message Passing:}$ There are several ways to share features from neighbor points. We can use MAX, AVG or SUM function. But the best method is use linear sum of the features $$f=\\sum\\limits_{i=1}^{6}\\alpha_if_i$$, with $\\alpha_i$ as learnable by the model. This $\\alpha_i$ is the attention score. It makes sure to give more weights during aggregation to points of similar nature or belonging to the same object. Figure 4: LFA Module Figure 4 is the detailed view of LFA module. It consists of three neural units (1) Local Spatial Encoding(LocSE) (2) Attentive Pooling (3) Dilated Residual Block. $\\textbf{A. Local Spatial Encoding}$\nLet $P=\\{p_1,p_2,\\cdots,p_n\\},p_i \\in \\mathbb{R}^3 \\text{ and } F=\\{f_1,f_2,\\cdots,f_n\\}, f_i \\in R^d$ be the point set and feature set accordingly. LSE units embed the features and the spatial information from the neighbourhood points. This helps the network learn the complex local geometrical structures with as increasing receptive field. For every point $p_i$, first K-Nearest Algorithm is used for finding $K$ neighbor points. Let the set of neighbor points, $N(p_i)=\\{p_1^{(i)},p_2^{(i)},\\cdots,p_K^{(i)}\\}$ and the set of features for the neighbor points be $N(f_i)=\\{f_1^{(i)},f_2^{(i)},\\cdots,f_K^{(i)}\\}$. At first positional features for every point in $N(p_i)$ is encoded as follows. (Figure 4) \\begin{equation} r_k^{(i)}=MLP\\bigg(p_i;p_k^{(i)};(p_i-p_k^{(i)});||p_i-p_k^{(i)}||\\bigg), r_k^{(i)} \\in \\mathbb{R}^r \\end{equation} $;$ is the concatenation layer and $||\\cdot||$ is the $l_2$ distance between neighbor and center points. $r_k^{(i)}$ not only just concatenates two positions but also the effect of one point on another point in terms of distance is also added. Once $r_k^{(i)} ,\\forall k=1,2,\\cdots,K$ is computed it is concatenated with corresponding features in $N(f_i)$. $$\\hat{F}=\\{\\hat{F}_1,\\hat{F}_2,\\cdots,\\hat{F}_i\\},\\hat{F_i}=\\{\\hat{f}_k^{(i)}\\}_{k=1}^{k=K}, \\hat{f}_k^{(i)}=\\{r_k^{(i)};f_k^{(i)}\\}$$ $\\textbf{B. Attentive Pooling}$\nAttentive pooling aggregates the set of neighboring point features $\\hat{F}$ with adaptive weights. Existing methods use mean or max pooling, resulting in the loss of important information. Attention mechanism will automatically learn important features. Given $\\hat{F_i}=\\{\\hat{f}_1^{(i)},\\cdots,\\hat{f}_k^{(i)}\\}$, first attention scores are computed using a shared MLP, $g$ such that \\begin{equation} s_k^{(i)}=g(\\hat{f}_k^{(i)},W) \\end{equation} where $W$ is the weight of the MLP. After learning the attention scores feature for point $p_i$, $f_i$ is updated with concatenated neighbor features. (Figure 4) \\begin{equation} \\hat{f}_i=MLP(\\sum\\limits_{k=1}^{K}(\\hat{f}_k^{(i)} \\odot s_k^{(i)})) \\end{equation} Together with LSE and Attentive pooling, the model learns informative features with geometric patterns for point $p_i$. $\\textbf{C. Dilated Residual Block}$ Since the point cloud is downsampled, it is necessary to expand the receptive field to preserve geometric details. Inspired by Resnet architecture, the author stacks several LSE and attentive pooling in one block before downsampling. In Figure 6, the red points observe $K$ features from neighboring points after the first LSE and Attentive Pooling layer and then in the next step it learns from $K^2$ features (See Figure 5). However, the more layers are added, the more the model is likely to be over-fitted. In the original paper (Figure 5), only two layers of LSE and Attentive pooling are used. Figure 5: Dilated Residual Block Figure 6: Illustration of dilated residual block which expands the receptive field at each step. 4. Conclusion $\\textbf{Advantages:}$\nThe main advantages of RandLa-Net are It is lightweight and achieves state-of-the-art results compared to existing methods. The random sampling method reduces the computation. The proposed attention-based Local Feature Aggregation (LFA) can expand into larger receptive fields using Local Spatial Encoding (LSE) with attentive pooling of point and neighbor features. The network consists of Shared MLP without any need of graph reconstruction or voxelization. The encoder-decoder architecture with downsampling aims to generate discriminative latent vectors using small samples which represent the objects of interest. $\\textbf{Disadvantages:}$\nThe random downsampling rate can influence the performance of the model. Reducing too many points will prevent the model from learning rich latent representations. Even though RandLaNet input allows addition of other features such as intensity, gradient, etc, it fails to learn local geometrical information. It learns the average shape of the object which causes over-segmentation. For more information, Thesis Report. ( Look at the Modified RandLa-Net with Feature Extractor and Voxel Segmentation Results) 5. Bibliography Anh Nguyen, Bac Le, 3D Point Cloud Segmentation - A Survey, 2013 6th IEEE Conference on Robotics, Automation and Mechatronics (RAM), 2013, pp. 225-230.\nCharles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas, PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 77-85.\nQingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, A. Trigoni, A. Markham, RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n","permalink":"/blog/randlanet/","tags":null,"title":"RandLaNet"},{"categories":["Anomaly","Autoencoder","Object Detection","SSD","Deep Learning","Image Reconstruction","Computer Vision"],"contents":" Abstract A strengthened autoencoder formed by placing an object detector upstream of a decoder is here developed in the context of the model-helped human analysis of composed ornaments from a dictionary of vignettes. The detection part is in charge to detect regions of interest containing some vignette features, and the decoding part to ensure vignette reconstruction with a relative quality depending on feature match. Images of ornaments without typographical composition are generated in order to properly assess the performance of each of the two parts.\n","permalink":"/publications/dae/","tags":null,"title":"Vignette detection and reconstruction of composed ornaments with a strengthened autoencoder"},{"categories":null,"contents":"** Contact page don\u0026rsquo;t contain a body, just the front matter above. See form.html in the layouts folder **\n","permalink":"/contact/","tags":null,"title":"Contact"},{"categories":["Image Annotation","Computer Vision","Object Detection"],"contents":" 1. Introduction Data Collection is a major step in every Machine Learning projects. The quality and quantity of data alone can greatly impact the results of the model. AutoLabelme is an open-source automatic image annotator created using the Tkinter library in Python. It\u0026rsquo;s an extension of LabelMe, the open-source Image Annotator available in Github LabelMe. It matches the template provided by the user in an image and find same objects associating a bounding box and label for every object. Autolabelme uses Normalized Cross-Correlation to check whether two templates are similar. It is designed keeping in mind with the necessity of creating dataset for object detection purposes and currently can only find objects in the cropped image i.e the search space to find same object is the space around the current template.\n2. Properties AutoLabelMe is simple to use. After LabelMe Manual Annotation just run AutoLabelMe. It is fast. The search space to the local neighbourhood of the template which makes it efficient for some projects. AutoLabelMe can identify rotated, scaled, horizontally and vertically flipped templates. There is no need for manually annotate rotated version of any template. It assigns a new label for the rotated templates with the rotaion information added in the label. AutoLabelMe also saves any meta information user stores during the LabelMe Manual Annotation step. For example, if for a bouding box, user assigns its label as \"0 This is a Sun\". AutoLabelMe will create a separate json file storing the new label for the bounding box and the meta information. This is helpful to save any additional information user wants for any object. 3. Tutorial Types of Windows Left Pane: Shows the image and matching for the current label. Red for the usual boxes. Blue for horizontally or vertically flipped boxes. Green for rotated. Although these are not final colors of the boxes. These are present only for checking. Top-Right Pane: Buttons. Bottom-Right Pane: Shows the image with all the matched templates. These image is helpful to recognise if every templates are matched or not. If any object is not annotated, it can be added using LabelMe. Functions of Every Buttons and Boxes Next Line $\u003e\u003e$: Template matching for next label. $\u003c\u003c$Previous Line: Template matching for previous label. $-$: Increases the threshold which results in less number of boxes. $+$: Decreses the threshold which results in detection of more boxes. Save Json: Saves a JSON file which can be read by LabelMe for further edits. Save Images: Save the cropped vignettes from the image. min: The minimum value of Rotation max: The maximum value of Rotation Rematch: Match again for the current label with the current setting. If min and max are provided, Rematch will match and activate rotation in template-matching. Correct Label:Transform all the boxes to red boxes (no flip). How to Run Open Terminal. Open Labelme. Write labelme or \"path/to/labelme\". Create one bounding box per label. Run AutoLabelme.py in Terminal python3 /path/to/AutoLabelme.py.. Open JSON and press Next Line $\u003e\u003e$ to start matching. The Left Pane will show the images with boxes for current Label. The smaller bottom-right pane is for showing the image with all the matched templates. Press $\u003c\u003c$Previous Line for viewing the matched boxes for the previous label. Press $+$ for more boxes and $-$ for less boxes. If you have rotated image, fill the rotation range or just enter min value. For example min=45 and max=90 will give the values 45,50,55,...90 or just enter min=45 which will only rotate the image once (45 degree). The search space value is by default 2 which means the algorithm will check for the templates from two heights up to two heights down in the original image. Choose any value from 1 to 15. For example if your template starts from coordinate (200,200) and height and width is 100 and 100 respectively and search space=2, the algorithm will search for the template from (0,0) to (400,400) in the image. If you select more than 15 than it's just heights-the value. For example, if you choose 100 in the previous example, the it will be (0,100) to (300,300) where the templates will be searched in the image. In any case, value from 1 to 15 is sufficient. Press Rematch button or Press Enter or Return in your keyboard. (Optional) Sometimes if the template is symmetric, the algorithm picks up some templates as flipped, to fix this, press Correct Label. Press Save Json to save a json file. Open the saved json file in Labelme. Labelme will show the matched templates. Edit it if necessary. Press Save Images in AutoLabelme if all the boxes are okay. This will save the matched templates in JPEG. 4. Future Work In V2, AutoLabelme will be used independently to annotate manually or automatically.\n","permalink":"/projects/autolabelme/","tags":null,"title":"AutoLabelMe"},{"categories":["Image Annotation","Computer Vision","Object Detection"],"contents":"A Detector-Encoder Autoencoder, has two parts (Figure 1) - (1) the detection part in the front, $\\Phi(F_{d}(x))$, where $F_{d}$ corresponds to an encoding function performing RoI detection (a Detector-Encoder dedicated to representation learning in the scope of object detection, see Figure 2 and where $\\Phi$ corresponds to a RoI Pooling transformation (Figure 4) (2) the reconstruction part, $F_{r}(\\phi)$, where $\\phi=\\Phi(F_{d}(x))$), a Decoder responsible for image reconstruction.\n1. Detection Part In AutoEncoders, the encoder part encodes any input image from which the decoder part reconstructs the image. DEA is designed to reproduce the detected objects. Conventional AutoEncoders designed to perform well copy-and-paste of inputs, can generalize too much in some abnormal cases. To address this issue, we suggest to replace the encoder with a Single Shot Multibox Detector (SSD)[1] using the following feature maps for the purpose of detection with backbone classifier VGG-16 (Figure 2): Figure 2: SSD Architecture [1] Conv4_3: Size $38\\times 38\\times 512$ Conv7: Size $19\\times 19\\times 1024$ Conv8_2: Size $10\\times 10\\times 512$ Conv9_2: Size $5\\times 5\\times 256$ Conv10_2: Size $3\\times 3\\times 256$ Conv11_2: Size $1\\times 1\\times 256$ To optimize detector SSD, it is suggested to use an Intersection over Union (IoU) loss in replacement of $l_{1}$ loss, namely a Generalized IOU [2], Distance-IoU [3] and Efficient IOU [4].\nThe feature maps encode the characteristics of the objects of the dictionary. For each of the feature maps, SSD considers 4 or 6 default boxes per cell (Total cell=$(38\\times 38 \\times 4)+(19\\times 19\\times6)+(10\\times10\\times6)+(5\\times5\\times6)+(3\\times3\\times4)+(1\\times1\\times4)=8732$). The final output of SSD for each of the default boxes are: (1) its offset $(\\delta x,\\delta y,\\delta w,\\delta h)$ (2) a probability vector $(p_{0},p_{1},\\cdots,p_{n})$ where $p_{i}$ is the probability that the box contains an object of the $i\\mathit{th}$ class. Class $i$ for all $i \\in\\{1,\\cdots,n\\}$, corresponds to the $i\\mathit{th}$ object in the training dataset whereas class $0$ refers to the image background. A Region of Interest (RoI) is determined from the final predicted box by maximizing the probability values, which provides a confidence value with respect to the fact that it contains a certain object. The corresponding features are accessible by checking the corresponding index (Figure 4) and this gives us the information of which feature maps the RoI is from. RoIs are like the objects, of different sizes and aspect ratios. By adding some layers devoted to RoI resizing after SSD, RoIs are ready to be properly processed by the reconstruction part. 1) Bounding Box Regression: Bounding box regression is one of the most important components in object detection tasks. In conventional SSD, a $l_{n}$ norm is used during training to evaluate the performance of the detector with the IoU (Intersection over Union) metric: $IoU=\\frac{|B_{G}\\cap B_{P}|}{|B_{G} \\cup B_{P}|}$ where $B_{g}$ and $B_{d}$ are the ground and predicted bounding boxes, respectively. However there is no correlation between minimizing $l_{n}$ norm and improving the loss associated to the IoU metric, $L_{IoU}=1-IoU(B_{g},B_{d})$. Figure 3: Three cases where the $l_{2}$-norm distance between the representations of two rectangular bounding boxes, each given by the concatenation of the coordinates of two opposite corners, has the same value but IoU and GIoU metrics have very different values [2] In Figure 3, the predicted bounding box (black rectangle) and ground truth box (green rectangle) are each represented by their top-left and bottom-right corners (pointed by arrows), and whose the Cartesian coordinates are denoted as $(x_{1} , y_{1} , x_{2} , y_{2})$ and $(x_{1}' , y_{1}' , x_{2}' , y_{2}')$, respectively. For simplicity, let us assume that the distance, e.g. $l_{2}-norm$, between one of the corners of two boxes is fixed. Now, if the second corner lies on a circle with fixed radius centered on the ground truth box, then the $l_{2}$ loss between the ground truth box and the predicted bounding box is the same although their IoU values can be different depending upon the positions of top-right and bottom-left corners. So, using IOU-loss should be the best option since a bad detector will impact negatively in the reconstruction part. However IoU has two major issues as a metric, following from its definition. If two boxes do not overlap, then their IoU is zero, which does not give any indication whether they are close or far. In addition, in case of non-overlapping boxes, since their IOU is zero, the gradient is also zero, and loss $L_{IoU}$ cannot be optimized. A variant of this loss was suggested to address the weaknesses of the IoU metric: the Generalized IoU loss Generalized IOU , $L_{GIoU}=1-GIoU$ given by the metric defined by $GIoU=IoU-\\frac{|C\\setminus (B_{g}\\cup B_{P})|}{|B_{p}|}$ where $C$ is the convex hull of the union of bounding boxes $B_{g}, B_{P}$. Computing efficient, approximate versions were later proposed in Distance-IoU: The Distance IOU loss defined as $L_{DIoU}=1-DIoU$ where $$DIoU=IoU-\\frac{\\rho^{2}(b_{g},b_{p})}{c_C^{2}}$$, $\\rho$ is the euclidean distance, and $c_C$ is the length of the diagonal of convex hull $C$, The Efficient IOU loss Efficient IOU defined as $L_{EIoU}=1-EIoU$ where $$EIoU=IoU-\\frac{\\rho^{2}(b_{g},b_{p})}{c^{2}} -\\frac{\\rho^{2}(w_{g},w_{p})}{c_{w}^{2}}-\\frac{\\rho^{2}(h_{g},h_{p})}{c_{h}^{2}}, \\rho$$ is the euclidean distance, and ($b,w,h$ defines a box centered in point $\\mathbf{b}$ having width $w$ and height $h$, its diagonal length being denoted as $c$). Figure 4: RoI Alignment step in DEA architecture. 2) RoI Alignment: We used RoIAlign, first proposed in , which allows the extraction of a $k\\times k$ RoI where $k$ is a predefined integer value, from feature maps. For any $N\\times N$ RoI, RoIAlign divides the feature maps into $k^2$, $\\frac{N}{k} \\times \\frac{N}{k}$ regions, named RoI bins, in each of which is computed a single value: the maximum or the average of the values at four points determined at the end by a linear interpolation. 2. Reconstruction Part We constructed the Decoder with three fully connected linear layers. The feature maps obtained from xIoU-SSD are first transformed into $1 \\times 1 \\times 1024$ using Average Pooling with kernel $(5,5)$. Each of the transformed feature maps are flattened and fed into the linear layers which are as described below (See Fig. 1): Layer 1: Input 1024 $\\rightarrow$ Output 2048 (Activation Function: Relu). Layer 2: Input 2048 $\\rightarrow$ Output 4096 (Activation Function: Relu). Layer 2: Input 2048 $\\rightarrow$ Output 4096 (Activation Function: Relu). Layer 3: Input 4096 $\\rightarrow$ Output 16384 (Activation Function: Sigmoid) The output, any reconstructed object, is then reshaped into a $128 \\times 128 \\times 1$ image. 3. Bibliography Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg. SSD: Singe Shot Detector. Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese. Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression. Zheng Z., et al\nDistance-IoU Loss: Faster and Better Learning for Bounding Box Regression Zhang Y., et al\nFocal and Efficient IOU Loss for Accurate Bounding Box Regression He. k, et al\nMask RCNN ","permalink":"/projects/dea/","tags":null,"title":"Detector-Encoder AutoEncoder"},{"categories":["Deep Learning","Computer Vision"],"contents":" 1. Why is $l_{n}$ not a good choice? Bounding Box Regression is the most important task in Object Detection. In conventional object detection networks, a $l_{n}$ norm is used during training to evaluate the performance of the detector with the IoU (Intersection over Union) metric: $$IoU=\\frac{|B_{G}\\cap B_{P}|}{|B_{G} \\cup B_{P}|}$$ where $B_{g}$ and $B_{d}$ are the ground and predicted bounding boxes, respectively. However there is no correlation between minimizing $l_{n}$ norm and improving the loss associated to the IoU metric [1], $$L_{IoU}=1-IoU(B_{g},B_{d})$$. Figure 1: Three cases where the $l_{2}$-norm distance between the representations of two rectangular bounding boxes, each given by the concatenation of the coordinates of two opposite corners, has the same value but IoU and GIoU metrics have very different values [1] In Figure 1, the predicted bounding box (black rectangle) and ground truth box (green rectangle) are each represented by their top-left and bottom-right corners (pointed by arrows), and whose the Cartesian coordinates are denoted as $(x_{1} , y_{1} , x_{2} , y_{2})$ and $(x_{1}' , y_{1}' , x_{2}' , y_{2}')$, respectively. For simplicity, let us assume that the distance, e.g. $l_{2}-norm$, between one of the corners of two boxes is fixed. Now, if the second corner lies on a circle with fixed radius centered on the ground truth box, then the $l_{2}$ loss between the ground truth box and the predicted bounding box is the same although their IoU values can be different depending upon the positions of top-right and bottom-left corners. So, using IOU-loss should be the best option since we will then minimize the evaluating metric. 2. IoU Loss [4] Generally, for two finite sample sets A and B, their IoU is defined as the intersection $(A \\cap B)$ divided by the union $(A \\cup B)$ of A and B. $IoU(A,B)=\\frac{|A \\cap B|}{|A \\cup B|}=\\frac{|A \\cap B|}{|A|+|B|-|A \\cup B|}$ For bounding box-level object detection, the target object is usually represented by a minimum Bbox rectangle in the 2D image. Base on this representation, the IoU computation between the ground bounding box $B_{g}=(x_{1} , y_{1} , x_{2} , y_{2} )$ and the predicted bounding box $B_{d}=(x_{1}^{\\prime} , y_{1}^{\\prime} , x_{2}^{\\prime} , y_{2}^{\\prime} )$ is defined as:- $ IoU(A,B)=\\frac{\\text{Area of overlap between $B_{g}$ and $B_{d}$}}{\\text{Area of union of $B_{g}$ and $B_{d}$}}=$ $\\frac{(max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime}))\\times (max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime}))}{(x_{2}-x_{1}) (y_{2}-y_{1})+(x_{2}^{\\prime}-x_{1}^{\\prime}) (y_{2}^{\\prime}-y_{1}^{\\prime})-(max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime})) (max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime}))}$ Usually, objects are labeled with axis-aligned BBoxes in the ROI dataset. By taking this kind of labels as ground truth, the predicted BBoxes are also axis-aligned rectangles. For this case, the IoU computation is very easy.\nA. Loss Function The IOU loss function[4] for the ground bounding box $B_{g}=(x_{1} , y_{1} , x_{2} , y_{2} )$ and the predicted bounding box $B_{d}=(x_{1}^{\\prime} , y_{1}^{\\prime} , x_{2}^{\\prime} , y_{2}^{\\prime} )$ is defined as $L_{IoU}=1-IoU(B_{g},B_{d})$ We have to prove that $L_{IoU}$ is a metric\nSince $0\\leq IoU \\leq 1, 0\\leq L_{IoU} \\leq 1$. So $L_{IoU}$ is non-negative. $L_{IoU}=0 \\text{ when } IoU(A,B)=1 \\implies$ $A$ and $B$ are the same rectangle. $IoU(A,B)=IoU(B,A) \\implies L_{IoU}(A,B)=L_{IoU}(B,A)$. So $L_{IoU}$ is symmetric. $L_{IoU}$ satisfies triangle inequality.[5] So $L_{IoU}$ is a metric. B. Differentiability of IoU Loss IOU loss is differentiable and can be backpropagated. Let $B_{g}=\\{x_{1},y_{1},x_{2},y_{2}\\}$ be the ground truth and $B_{d}=\\{x_{1}^{\\prime} , y_{1}^{\\prime} , x_{2}^{\\prime} , y_{2}^{\\prime}\\}$ be the predicted bounding box.\n$X= \\text{Area of } B_{g}=(x_{2}-x_{1}) \\times (y_{2}-y_{1})$ $X^{\\prime}= \\text{Area of } B_{d}=(x_{2}^{\\prime}- x_{1}^{\\prime})\\times (y_{2}^{\\prime}-y_{1}^{\\prime})$ $I=(max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime})) \\times (max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime}))$ $L_{IoU}=1-\\frac{I}{X+X^{\\prime}-I}=1-\\frac{I}{U}, \\text{where } U=X+X^{\\prime}-I$ $\\frac{\\partial L}{\\partial x^{\\prime}}=\\frac{I(\\Delta_{x^{\\prime}}X- \\Delta_{x^{\\prime}}I)-U\\Delta_{x^{\\prime}}I}{U^{2}}$ $\\frac{\\partial X}{\\partial x_{1}^{\\prime}}=- (y_{2}^{\\prime}-y_{1}^{\\prime}), \\frac{\\partial X}{\\partial x_{2}^ {\\prime}}=(y_{2}^{\\prime}-y_{1}^{\\prime}),\\frac{\\partial X} {\\partial y_{2}^{\\prime}}=(x_{2}^{\\prime}-x_{1}^{\\prime}), \\frac{\\partial X}{\\partial y_{1}^{\\prime}}=-(x_{2}^{\\prime}-x_{1}^ {\\prime})$ $\\frac{\\partial I}{\\partial x_{1}^{\\prime}}= \\begin{cases} (max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime})) \u0026 \\text{ if } x_{1}^{\\prime}\u003ex_{1}\\\\ 0 \u0026 { Otherwise } \\end{cases}$ $\\frac{\\partial I}{\\partial x_{2}^{\\prime}}= \\begin{cases} -(max(y_{1},y_{1}^{\\prime})-min(y_{2},y_{2}^{\\prime})) \u0026 \\text{ if } x_{2}\u003ex_{2}^{\\prime}\\\\ 0 \u0026 { Otherwise } \\end{cases} $ $\\frac{\\partial I}{\\partial y_{1}^{\\prime}}= \\begin{cases} (max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime})) \u0026 \\text{ if } y_{1}^{\\prime}\u003ey_{1}\\\\ 0 \u0026 { Otherwise } \\end{cases}$ $\\frac{\\partial I}{\\partial y_{2}^{\\prime}}= \\begin{cases} -(max(x_{1},x_{1}^{\\prime})-min(x_{2},x_{2}^{\\prime})) \u0026 \\text{ if } y_{2}\u003ey_{2}^{\\prime}\\\\ 0 \u0026 { Otherwise } \\end{cases}$ So $L_{IoU}$ can be directly used as the objective function to optimize. It is therefore preferable to use IoU as the objective function for 2D object detection tasks. Given the choice between optimizing a metric itself vs.a surrogate loss function, the optimal choice is the metric itself. 3. GIoU Loss However, IOU has two major issues as a metric and loss function.\nIf two boxes don\u0026rsquo;t overlap, then their IoU is zero, which doesn\u0026rsquo;t give any indication about the proximity of the two boxes. In case of non-overlapping boxes, since their Iou is zero, the gradient is also zero. So $L_{IoU}$ can\u0026rsquo;t be optimized. Generalized IoU (GIoU) addresses these weaknesses of IoU. A. Loss Function The Loss function for the Generalized IoU is defined as follows:-\nLet A and B be two boxes. Let C be the smallest enclosing box. $IoU=\\frac{|A\\cap B|}{|A \\cup B|}$. $GIoU=IoU-\\frac{|C\\setminus (A\\cup B)|}{|C|}$. $L_{GIoU}=1-GIoU$. Some of the properties of GIoU[1]:- Similar to $L_{IoU}, L_{GIoU}$ is also non-negative, symmetric and satisfies triangle inequality. So $L_{GIoU}$ is a metric.\nGIoU is a lower bound for IoU. $\\forall A,B, GIoU(A,B)\\leq IoU(A,B)$ and this lower bound becomes tighter when A and B have a stronger shape similarity i.e $\\lim_{A \\to B} GIoU(A,B)=IoU(A,B)$.\n$0\\leq IoU(A,B)\\leq 1 \\implies -1\\leq GIoU(A,B)\\leq 1$.\n$GIoU=1 \\text{ when } |A\\cap B|=|A\\cup B|$ i.e when A and B completely overlaps.\nGIoU tends to -1 as the ratio between occupying regions $|A\\cup B|$ and the smallest enclosing box C goes to zero, i.e $\\lim_{\\frac{|A\\cup B|}{C} \\to 0} GIoU(A,B)=-1 $\n$L_{GIoU}$ is differentiable.\nWhen IoU=0, i.e boxes don\u0026rsquo;t overlap, $L_{GIoU}=2-\\frac{|A\\cup B|}{|C|}$. By minimizing $L_{GIoU}$, we are maximizing $\\frac{|A\\cup B|}{|C|}$ ($0\\leq \\frac{|A\\cup B|}{|C|} \\leq 1$) which means we are maximizing the region of union $|A\\cup B|$ and minimizing the enclosing box area $|C|$, which will be possible if the predicted box goes to the ground truth box.\n4. DIoU and CIoU Loss IoU loss works only for overlapping boxes and the problem of gradient-vaninshing in case of non-overlapping boxes had been solved by GIoU Loss but GIoU loss has several limitations.\nGeneralized IoU tends to increase the size of the predicted bounding box to cover the target ground truth box. From Figure 2, we can see that when the predicted bounding box covers the ground truth box then $ L_{GIoU}=L_{IoU}$ (Since C=$max(A,B)\\implies C\\setminus(A\\cup B)=\\Phi$). $L_{GIoU}$ converges slowly. Figure 2: The green, black, blue, red represents the ground truth box, the anchor box, the predicted box at ith step when GIoU loss is used,the predicted box at ith step when DIoU loss is used respectively. GIoU tends to extend the box to cover the [2] A. Distance IoU (DIoU) Loss Function[2] Generally IoU-based loss functions can be defined as $$L=1-IoU+R(B,B^{gt})$$\nwhere $R(B,B^{gt})$ is a penalty term and $B_{gt}$ and $B$ are the ground truth box and predicted box. DIoU minimizes the normalized distance between the centre point of the two bounding boxes. The penalty term is $$R_{DIoU}=\\frac{\\rho^{2}(b,b_{gt})}{c^{2}}$$\nwhere $b$ and $b_{gt}$ denote the central points of $B$ and $B_{gt}$, and $c$ is the diagonal length of the smallest enclosing box. and $\\rho$ is the euclidean distance. So, $$L_{DIoU}=1-IoU+\\frac{\\rho^{2}(b,b_{gt})}{c^{2}}$$\n$L_{DIoU}$ is scale invariant. $L_{IoU}=L_{GIoU}=L_{DIoU}=0$ when two boxes are same. B. Complete IoU (CIoU) Loss Function[2] Complete IoU is based upon DIoU loss and considers the aspect ratio factor. The loss function for Complete IoU Loss function is $$L_{CIoU}=L_{DIoU}+\\alpha v$$\nwhere $v=\\frac{4}{\\pi^2}(tan^{-1}\\frac{w^{gt}}{h^{gt}} -tan^{-1}\\frac{w}{h})^2 $ and $\\alpha=\\frac{v}{(1-IOU)+v}$.\n5. Code import os import torch import math import torch.nn as nn import torchvision def calculate_iou(pred,true): \u0026#34;\u0026#34;\u0026#34;Functions to calculate IoU\u0026#34;\u0026#34;\u0026#34; ints_x_min=torch.max(true[:,0],pred[:,0]) ints_y_min=torch.max(true[:,1],pred[:,1]) ints_x_max=torch.min(true[:,2],pred[:,2]) ints_y_max=torch.min(true[:,3],pred[:,3]) width=torch.max((ints_x_max-ints_x_min),torch.tensor([0]).unsqueeze(0)) height=torch.max((ints_y_max-ints_y_min),torch.tensor([0]).unsqueeze(0)) area_intersection=torch.max(width*height, torch.tensor([0]).unsqueeze(0)) # Find Area of the Box True area_true=torch.mul((true[:,2]-true[:,0]),(true[:,3]-true[:,1])) # Find Area of the Box Pred area_pred=torch.mul((pred[:,2]-pred[:,0]),(pred[:,3]-pred[:,1])) # Find Area of the Union area_union=area_true+area_pred-area_intersection # Calculate IoU iou=area_intersection/area_union return iou,area_intersection,area_union class IoULoss(nn.Module): \u0026#34;\u0026#34;\u0026#34;Intersection over Union Loss\u0026#34;\u0026#34;\u0026#34; def __init__(self,losstype=\u0026#39;giou\u0026#39;): super(IoULoss, self).__init__() \u0026#34;\u0026#34;\u0026#34;losstype --\u0026gt; str. Type of IoU based Loss. \u0026#34;iou\u0026#34;,\u0026#34;giou\u0026#34;,\u0026#34;diou\u0026#34;,\u0026#34;ciou\u0026#34;,\u0026#34;eiou\u0026#34; are available\u0026#34;\u0026#34;\u0026#34; self.losstype =losstype def forward(self, pred, true): pred=torch.clamp(pred,min=0) true=torch.clamp(true,min=0) if self.losstype == \u0026#34;iou\u0026#34;: loss=torch.mean(1-calculate_iou(pred,true)[0]) elif self.losstype == \u0026#34;giou\u0026#34;: l_giou=1-calculate_iou(pred,true)[0]+self.penalty_giou(pred,true) loss=torch.mean(l_giou) elif self.losstype == \u0026#34;diou\u0026#34;: l_diou=1-calculate_iou(pred,true)[0]+self.penalty_diou(pred,true) loss=torch.mean(l_diou) elif self.losstype == \u0026#34;ciou\u0026#34;: l_ciou=1-calculate_iou(pred,true)[0]+self.penalty_ciou(pred,true) loss=torch.mean(l_ciou) elif self.losstype == \u0026#34;eiou\u0026#34;: l_eiou=1-calculate_iou(pred,true)[0]+self.penalty_eiou(pred,true) loss=torch.mean(l_eiou) return loss def penalty_giou(self, pred, true): # Find Area of the Smallest Enclosing Box box_x_min=torch.min(true[:,0],pred[:,0]) box_y_min=torch.min(true[:,1],pred[:,1]) box_x_max=torch.max(true[:,2],pred[:,2]) box_y_max=torch.max(true[:,3],pred[:,3]) area_c=(box_x_max-box_x_min)*(box_y_max-box_y_min) return (area_c-calculate_iou(pred,true)[2])/area_c def penalty_diou(self, pred, true): # Center point of the predicted bounding box center_x1 = (pred[:, 2] + pred[:, 0]) / 2 center_y1 = (pred[:, 3] + pred[:, 1]) / 2 # Center Point of the ground truth box center_x2 = (true[:, 2] + true[:, 0]) / 2 center_y2 = (true[:, 3] + true[:, 1]) / 2 inter_max_xy = torch.min(pred[:, 2:],true[:, 2:]) inter_min_xy = torch.max(pred[:, :2],true[:, :2]) # Bottom right corner of the enclosing box out_max_xy = torch.max(pred[:, 2:],true[:, 2:]) # Top left corner of the enclosing box out_min_xy = torch.min(pred[:, :2],true[:, :2]) # Distance between the center points of the ground truth and the predicted box inter_diag = (center_x2 - center_x1)**2 + (center_y2 - center_y1)**2 outer = torch.clamp((out_max_xy - out_min_xy), min=0) outer_diag = (outer[:, 0] ** 2) + (outer[:, 1] ** 2) return inter_diag/outer_diag def penalty_ciou(self, pred, true): w1 = pred[:, 2] - pred[:, 0] h1 = pred[:, 3] - pred[:, 1] w2 = true[:, 2] - true[:, 0] h2 = true[:, 3] - true[:, 1] v = (4 / (math.pi ** 2)) * torch.pow((torch.atan(w2 / h2) - torch.atan(w1 / h1)), 2) with torch.no_grad(): S = 1 - calculate_iou(pred,true)[0] alpha = v / (S + v) return self.penalty_diou(pred,true)+alpha*v def penalty_eiou(self, pred, true): w1 = pred[:, 2] - pred[:, 0] h1 = pred[:, 3] - pred[:, 1] w2 = true[:, 2] - true[:, 0] h2 = true[:, 3] - true[:, 1] # Bottom right corner of the enclosing box out_max_xy = torch.max(pred[:, 2:],true[:, 2:]) # Top left corner of the enclosing box out_min_xy = torch.min(pred[:, :2],true[:, :2]) # Width of the Smallest enclosing box C_w=(out_max_xy[:,0]-out_min_xy[:,0]) # Height of the smallest enclosing box C_h=(out_max_xy[:,1]-out_min_xy[:,1]) asp= torch.clamp((w2-w1)**2,min=0)/(C_w**2) + torch.clamp((h2-h1)**2,min=0)/(C_h**2) return self.penalty_diou(pred,true)+asp 6. Bibliography Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese.\nGeneralized Intersection over Union: A Metric and A Loss for Bounding Box Regression. Zheng Z., et al\nDistance-IoU Loss: Faster and Better Learning for Bounding Box Regression Zhang Y., et al\nFocal and Efficient IOU Loss for Accurate Bounding Box Regression J Yu, et al\nUnitBox: An Advanced Object Detection Network S Kosub A note on the triangle inequality for the Jaccard distance ","permalink":"/blog/iouloss/","tags":null,"title":"IoU Losses"},{"categories":null,"contents":"My blogs are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n","permalink":"/license/","tags":null,"title":"License"}]